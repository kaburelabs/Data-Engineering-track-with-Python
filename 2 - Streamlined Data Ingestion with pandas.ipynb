{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1 - Streamlined Data Ingestion with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from CSVs\n",
    "\n",
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV and assign it to the variable data\n",
    "data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "\n",
    "# View the first few lines of data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from other flat files\n",
    "\n",
    "# Import pandas with the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load TSV using the sep keyword argument to set delimiter\n",
    "data = pd.read_csv('vt_tax_data_2016.tsv', sep='\\t')\n",
    "\n",
    "# Plot the total number of tax returns by income group\n",
    "counts = data.groupby(\"agi_stub\").N1.sum()\n",
    "counts.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import a subset of columns\n",
    "\n",
    "# Create list of columns to use\n",
    "cols = ['zipcode', 'agi_stub', 'mars1', 'MARS2', 'NUMDEP']\n",
    "\n",
    "# Create data frame from csv using only selected columns\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
    "\n",
    "# View counts of dependents and tax returns by income level\n",
    "print(data.groupby(\"agi_stub\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import a file in chunks\n",
    "\n",
    "# Create data frame of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                       \t\t  nrows=500,\n",
    "                       \t\t  skiprows=500,\n",
    "                       \t\t  header=None,\n",
    "                       \t\t  names=list(vt_data_first500))\n",
    "\n",
    "# View the Vermont data frames to confirm they're different\n",
    "print(vt_data_first500.head())\n",
    "print(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify data types\n",
    "\n",
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub':'category',\n",
    "\t\t\t  'zipcode':str}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set custom NA values\n",
    "\n",
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode': 0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                   na_values=null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data.zipcode.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Skip bad data\n",
    "\n",
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False, \n",
    "                     warn_bad_lines=True)\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 - Importing Data From Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from a spreadsheet\n",
    "\n",
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read spreadsheet and assign it to survey_responses\n",
    "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
    "\n",
    "# View the head of the data frame\n",
    "print(survey_responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a portion of a spreadsheet\n",
    "\n",
    "# Create string of lettered columns to load\n",
    "col_string = 'AD, AW:BA'\n",
    "\n",
    "# Load data with skiprows and usecols set\n",
    "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
    "                        skiprows=2, \n",
    "                        usecols=col_string)\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select a single sheet\n",
    "\n",
    "# Create df from second worksheet by referencing its position\n",
    "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                               sheet_name=1)\n",
    "\n",
    "# Graph where people would like to get a developer job\n",
    "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "job_prefs.plot.barh()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create df from second worksheet by referencing its position\n",
    "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                               sheet_name='2017')\n",
    "\n",
    "# Graph where people would like to get a developer job\n",
    "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "job_prefs.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select multiple sheets\n",
    "\n",
    "\n",
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name=['2016', '2017'])\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())\n",
    "\n",
    "##################\n",
    "\n",
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name=[0, '2017'])\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())\n",
    "\n",
    "####################\n",
    "\n",
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name=None)\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Work with multiple spreadsheets\n",
    "\n",
    "# Create an empty data frame\n",
    "all_responses = pd.DataFrame()\n",
    "\n",
    "# Set up for loop to iterate through values in responses\n",
    "for df in responses.values():\n",
    "  # Print the number of rows being added\n",
    "  print(\"Adding {} rows\".format(df.shape[0]))\n",
    "  # Append df to all_responses, assign result\n",
    "  all_responses = all_responses.append(df)\n",
    "\n",
    "# Graph employment statuses in sample\n",
    "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
    "counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Boolean columns\n",
    "\n",
    "# Set dtype to load appropriate column(s) as Boolean data\n",
    "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
    "                            dtype={'HasDebt':bool})\n",
    "\n",
    "# View financial burdens by Boolean group\n",
    "print(survey_data.groupby('HasDebt').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set custom true/false values\n",
    "\n",
    "# Load file with Yes as a True value and No as a False value\n",
    "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "                              dtype={\"HasDebt\": bool,\n",
    "                              \"AttendedBootCampYesNo\": bool},\n",
    "                              true_values=['Yes'],\n",
    "                              false_values=['No'])\n",
    "\n",
    "# View the data\n",
    "print(survey_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse simple dates\n",
    "\n",
    "# Load file, with Part1StartTime parsed as datetime data\n",
    "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                            parse_dates=['Part1StartTime'])\n",
    "\n",
    "# Print first few values of Part1StartTime\n",
    "print(survey_data.Part1StartTime.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get datetimes from multiple columns\n",
    "\n",
    "# Create dict of columns to combine into new datetime column\n",
    "datetime_cols = {\"Part2Start\": ['Part2StartTime', 'Part2StartDate']}\n",
    "\n",
    "\n",
    "# Load file, supplying the dict to parse_dates\n",
    "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "                            parse_dates=datetime_cols)\n",
    "\n",
    "# View summary statistics about Part2Start\n",
    "print(survey_data.Part2Start.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse non-standard date formats\n",
    "\n",
    "# Parse datetimes and assign result back to Part2EndTime\n",
    "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
    "                                             format=\"%m%d%Y %H:%M:%S\")\n",
    "\n",
    "# Print first few values of Part2EndTime\n",
    "print(survey_data[\"Part2EndTime\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 - Importing Data from Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to a database\n",
    "\n",
    "# Import sqlalchemy's create_engine() function\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# View the tables in the database\n",
    "print(engine.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load entire tables\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Create a SQL query to load the entire weather table\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Load weather with the SQL query\n",
    "weather = pd.read_sql(query, engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting columns with SQL\n",
    "\n",
    "# Create database engine for data.db\n",
    "engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Write query to get date, tmax, and tmin from weather\n",
    "query = \"\"\"\n",
    "SELECT date, \n",
    "       tmax, \n",
    "       tmin\n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Make a data frame by passing query and engine to read_sql()\n",
    "temperatures = pd.read_sql(query, engine)\n",
    "\n",
    "# View the resulting data frame\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting rows\n",
    "\n",
    "# Create query to get hpd311calls records about safety\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM hpd311calls\n",
    "WHERE complaint_type=='SAFETY';\n",
    "\"\"\"\n",
    "\n",
    "# Query the database and assign result to safety_calls\n",
    "safety_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of safety calls by borough\n",
    "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
    "call_counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering on multiple conditions\n",
    "\n",
    "# Create query for records with max temps <= 32 or snow >= 1\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "  FROM weather\n",
    "  WHERE tmax<=32\n",
    "  OR snow>=1;\n",
    "\"\"\"\n",
    "\n",
    "# Query database and assign result to wintry_days\n",
    "wintry_days = pd.read_sql(query, engine)\n",
    "\n",
    "# View summary stats about the temperatures\n",
    "print(wintry_days.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting distinct values\n",
    "\n",
    "# Create query for unique combinations of borough and complaint_type\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT borough, \n",
    "       complaint_type\n",
    "  FROM hpd311calls;\n",
    "\"\"\"\n",
    "\n",
    "# Load results of query to a data frame\n",
    "issues_and_boros = pd.read_sql(query, engine)\n",
    "\n",
    "# Check assumption about issues and boroughs\n",
    "print(issues_and_boros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting in groups\n",
    "\n",
    "# Create query to get call counts by complaint_type\n",
    "query = \"\"\"\n",
    "SELECT complaint_type, \n",
    "     COUNT(*)\n",
    "  FROM hpd311calls\n",
    "  GROUP BY complaint_type;\n",
    "\"\"\"\n",
    "\n",
    "# Create data frame of call counts by issue\n",
    "calls_by_issue = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of calls for each housing issue\n",
    "calls_by_issue.plot.barh(x=\"complaint_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working with aggregate functions\n",
    "\n",
    "# Create query to get temperature and precipitation by month\n",
    "query = \"\"\"\n",
    "SELECT month, \n",
    "        MAX(tmax), \n",
    "        MIN(tmin),\n",
    "        SUM(prcp)\n",
    "  FROM weather \n",
    " GROUP BY month;\n",
    "\"\"\"\n",
    "\n",
    "# Get data frame of monthly weather stats\n",
    "weather_by_month = pd.read_sql(query, engine)\n",
    "\n",
    "# View weather stats by month\n",
    "print(weather_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining tables\n",
    "\n",
    "# Query to join weather to call records by date columns\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM hpd311calls\n",
    "  JOIN  weather\n",
    "  ON hpd311calls.created_date = weather.date\n",
    "\"\"\"\n",
    "\n",
    "# Create data frame of joined tables\n",
    "calls_with_weather = pd.read_sql(query, engine)\n",
    "\n",
    "# View the data frame to make sure all columns were joined\n",
    "print(calls_with_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining and filtering\n",
    "\n",
    "# Query to get water leak calls and daily precipitation\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.*, weather.prcp\n",
    "  FROM hpd311calls\n",
    "  JOIN weather\n",
    "    ON hpd311calls.created_date = weather.date\n",
    "  WHERE hpd311calls.complaint_type == 'WATER LEAK';\n",
    "\"\"\"\n",
    "\n",
    "# Load query results into the leak_calls data frame\n",
    "leak_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# View the data frame\n",
    "print(leak_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining, filtering, and aggregating\n",
    "\n",
    "# Query to get heat/hot water call counts by created_date\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.created_date, \n",
    "       COUNT(*)\n",
    "  FROM hpd311calls \n",
    "  WHERE hpd311calls.complaint_type = 'HEAT'\n",
    "  GROUP BY hpd311calls.created_date;\"\"\"\n",
    "\n",
    "# Query database and save results as df\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# View first 5 records\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining, filtering, and aggregating\n",
    "\n",
    "# Modify query to join tmax and tmin from weather by date\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.created_date, \n",
    "\t   COUNT(*), \n",
    "       weather.tmax,\n",
    "       weather.tmin\n",
    "  FROM hpd311calls \n",
    "       JOIN weather\n",
    "       ON hpd311calls.created_date = weather.date\n",
    " WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' \n",
    " GROUP BY hpd311calls.created_date;\"\"\"\n",
    "\n",
    "# Query database and save results as df\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# View first 5 records\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4 - Load JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load JSON data\n",
    "\n",
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a data frame\n",
    "pop_in_shelters = pd.read_json('dhs_daily_report.json')\n",
    "\n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Work with JSON orientations\n",
    "\n",
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient='split')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient='split')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from an API\n",
    "\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a data frame\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set API parameters\n",
    "\n",
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {'term': 'cafe',\n",
    "          \t  'location':'NYC'}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "                        params=parameters,\n",
    "                        headers=headers)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print head\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set request headers\n",
    "\n",
    "# Create dictionary that passes Authorization and key string\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url, params=params, headers=headers)\n",
    "\n",
    "\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print names\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flatten nested JSONs\n",
    "\n",
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a data frame, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "                        sep='_')\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handle deeply nested data\n",
    "\n",
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta=['name', \n",
    "                                  'alias',  \n",
    "                                  'rating',\n",
    "                          \t\t  ['coordinates', 'latitude'], \n",
    "                          \t\t  ['coordinates', 'longitude']],\n",
    "                    \t\tmeta_prefix='biz_')\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append data frames\n",
    "\n",
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          'offset':50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes, ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge data frames\n",
    "\n",
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk,\n",
    "                                left_on='location_zip_code',\n",
    "                                right_on='zipcode')\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop = cafes_with_pumas.merge(pop_data, on='puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.0",
   "language": "python",
   "name": "tf_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
