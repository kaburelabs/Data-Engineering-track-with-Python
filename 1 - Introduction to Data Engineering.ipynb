{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 2 - Data engineering toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The database schema\n",
    "\n",
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining on relations\n",
    "\n",
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the id column of data\n",
    "print(data.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From task to subtasks\n",
    "\n",
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using a DataFrame\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of pratitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A PySpark groupby\n",
    "\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Running PySpark files\n",
    "\n",
    "repl:~$ cat /home/repl/spark-script.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    athlete_events_spark = (spark\n",
    "        .read\n",
    "        .csv(\"/home/repl/datasets/athlete_events.csv\",\n",
    "             header=True,\n",
    "             inferSchema=True,\n",
    "             escape='\"'))\n",
    "\n",
    "    athlete_events_spark = (athlete_events_spark\n",
    "        .withColumn(\"Height\",\n",
    "                    athlete_events_spark.Height.cast(\"integer\")))\n",
    "\n",
    "    print(athlete_events_spark\n",
    "        .groupBy('Year')\n",
    "        .mean('Height')\n",
    "        .orderBy('Year')\n",
    "        .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Airflow DAGs\n",
    "\n",
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\n",
    "                        \"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", \n",
    "                              bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", \n",
    "                           bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\",\n",
    "                             bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\",\n",
    "                           bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Extract, Transform and Load (ETL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch from an API\n",
    "\n",
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()['score']\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read from a database\n",
    "\n",
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the film table into a pandas DataFrame\n",
    "extract_table_to_pandas('film', db_engine)\n",
    "\n",
    "# Extract the customer table into a pandas DataFrame\n",
    "extract_table_to_pandas('customer', db_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the rental rate\n",
    "\n",
    "# Get the rental rate column as a string\n",
    "rental_rate_str = film_df.rental_rate.astype(str)\n",
    "\n",
    "# Split up and expand the column\n",
    "rental_rate_expanded = rental_rate_str.str.split('.', expand=True)\n",
    "\n",
    "# Assign the columns to film_df\n",
    "film_df = film_df.assign(\n",
    "    rental_rate_dollar=rental_rate_expanded[0],\n",
    "    rental_rate_cents=rental_rate_expanded[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining with ratings\n",
    "\n",
    "# Use groupBy and mean to aggregate the column\n",
    "ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')\n",
    "\n",
    "# Join the tables using the film_id column\n",
    "film_df_with_ratings = film_df.join(\n",
    "    ratings_per_film_df,\n",
    "    film_df.film_id==ratings_per_film_df.film_id\n",
    ")\n",
    "\n",
    "# Show the 5 first results\n",
    "print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing to a file\n",
    "\n",
    "# Write the pandas DataFrame to parquet\n",
    "film_pdf.to_parquet('films_pdf.parquet')\n",
    "\n",
    "# Write the PySpark DataFrame to parquet\n",
    "film_sdf.write.parquet('films_sdf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load into Postgres\n",
    "\n",
    "# Finish the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Transformation step, join with recommendations data\n",
    "film_pdf_joined = film_pdf.join(recommendations)\n",
    "\n",
    "# Finish the .to_sql() call to write to store.film\n",
    "film_pdf_joined.to_sql(\"film\", db_engine_dwh, schema=\"store\", if_exists=\"replace\")\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_dwh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a DAG\n",
    "\n",
    "# Define the ETL function\n",
    "def etl():\n",
    "    film_df = extract_film_to_pandas()\n",
    "    film_df = transform_rental_rate(film_df)\n",
    "    load_dataframe_to_film(film_df)\n",
    "\n",
    "# Define the ETL task using PythonOperator\n",
    "etl_task = PythonOperator(task_id='etl_heart_disease',\n",
    "                          python_callable=etl,\n",
    "                          dag=dag)\n",
    "\n",
    "# Set the upstream to wait_for_table and sample run etl()\n",
    "etl_task.set_upstream(wait_for_table)\n",
    "etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Case Study: DataCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Querying the table\n",
    "\n",
    "# Complete the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/datacamp_application\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Get user with id 4387\n",
    "user1 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=4387\", db_engine)\n",
    "\n",
    "# Get user with id 18163\n",
    "user2 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=18163\", db_engine)\n",
    "\n",
    "# Get user with id 8770\n",
    "user3 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=8770\", db_engine)\n",
    "\n",
    "# Use the helper function to compare the 3 users\n",
    "print_user_comparison(user1, user2, user3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average rating per course\n",
    "\n",
    "# Complete the transformation function\n",
    "def transform_avg_rating(rating_data):\n",
    "  # Group by course_id and extract average rating per course\n",
    "  avg_rating = rating_data.groupby('course_id').rating.mean()\n",
    "  # Return sorted average ratings per course\n",
    "  sort_rating = avg_rating.sort_values(ascending=False).reset_index()\n",
    "  return sort_rating\n",
    "\n",
    "# Extract the rating data into a DataFrame    \n",
    "rating_data = extract_rating_data(db_engines)\n",
    "\n",
    "# Use transform_avg_rating on the extracted data and print results\n",
    "avg_rating_data = transform_avg_rating(rating_data)\n",
    "print(avg_rating_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter out corrupt data\n",
    "\n",
    "course_data = extract_course_data(db_engines)\n",
    "\n",
    "# Print out the number of missing values per column\n",
    "print(course_data.isnull().sum())\n",
    "\n",
    "# The transformation should fill in the missing values\n",
    "def transform_fill_programming_language(course_data):\n",
    "    imputed = course_data.fillna({\"programming_language\": \"r\"})\n",
    "    return imputed\n",
    "\n",
    "transformed = transform_fill_programming_language(course_data)\n",
    "\n",
    "# Print out the number of missing values per column of transformed\n",
    "print(transformed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the recommender transformation\n",
    "\n",
    "# Complete the transformation function\n",
    "def transform_recommendations(avg_course_ratings, courses_to_recommend):\n",
    "    # Merge both DataFrames\n",
    "    merged = courses_to_recommend.merge(avg_course_ratings) \n",
    "    # Sort values by rating and group by user_id\n",
    "    grouped = merged.sort_values(\"rating\", ascending = False).groupby('user_id')\n",
    "    # Produce the top 3 values and sort by user_id\n",
    "    recommendations = grouped.head(3).sort_values(\"user_id\").reset_index()\n",
    "    final_recommendations = recommendations[[\"user_id\", \"course_id\",\"rating\"]]\n",
    "    # Return final recommendations\n",
    "    return final_recommendations\n",
    "\n",
    "# Use the function with the predefined DataFrame objects\n",
    "recommendations = transform_recommendations(avg_course_ratings, courses_to_recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The target table\n",
    "\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "def load_to_dwh(recommendations):\n",
    "    recommendations.to_sql(\"recommendations\", db_engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining DAGs\n",
    "\n",
    "# Define the DAG so it runs on a daily basis\n",
    "dag = DAG(dag_id=\"recommendations\",\n",
    "          schedule_interval='0 0 * * *')\n",
    "\n",
    "# Make sure `etl()` is called in the operator. Pass the correct kwargs.\n",
    "task_recommendations = PythonOperator(\n",
    "    task_id=\"recommendations_task\",\n",
    "    python_callable=etl,\n",
    "    op_kwargs={\"db_engines\": db_engines},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Querying the recommendations\n",
    "\n",
    "def recommendations_for_user(user_id, threshold=4.5):\n",
    "    # Join with the courses table\n",
    "    query = \"\"\"\n",
    "            SELECT title, rating FROM recommendations\n",
    "            INNER JOIN courses ON courses.course_id = recommendations.course_id\n",
    "            WHERE user_id=%(user_id)s AND rating>%(threshold)s\n",
    "            ORDER BY rating DESC\n",
    "        \"\"\"\n",
    "    # Add the threshold parameter\n",
    "    predictions_df = pd.read_sql(query, db_engine, params = {\"user_id\": user_id, \n",
    "                                                           \"threshold\": threshold})\n",
    "    return predictions_df.title.values\n",
    "\n",
    "# Try the function you created\n",
    "print(recommendations_for_user(12, 4.65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.0",
   "language": "python",
   "name": "tf_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
